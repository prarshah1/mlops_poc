name: CI pipeline
# our diagram in
# integrate with dvc with mlflow
# move this to new registry
# remove any references - complete new
# implement shap pdp and subpopulation with model training and store the artifacts in mlflow, lift- roc, auc, confusion matrix


# infer - load mlflow.pyfunc, or initialize pyspark and load

#model = mlflow.pyfunc.load_model(model_path)model.predict(model_input)
#
## load input data table as a Spark DataFrameinput_data = spark.table(input_table_name)model_udf = mlflow.pyfunc.spark_udf(model_path)df = input_data.withColumn("prediction", model_udf())


on:
  pull_request:
    branches:
      - main
    tags-ignore:
      - 'v*' # this tag type is used for release pipelines hk

jobs:
  ci-pipeline:

    runs-on: ubuntu-latest
    strategy:
      max-parallel: 4

    env:
#      DATABRICKS_HOST: https://dbc-79ee8895-9a53.cloud.databricks.com
#      DATABRICKS_TOKEN: dapib7b9ee9e57d9d622c5f14a9b343fdf3b
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_PROD_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_PROD_TOKEN }}

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: 3.9.5

      - name: Install pip
        run: |
          python -m pip install --upgrade pip

      - name: Install dependencies and project in dev mode
        run: |
          pip install -r unit-requirements.txt
          pip install -e .

      - name: Run dvc
        run: |
          dvc pull
          dvc repro
          dvc push

      - name: Run unit tests
        run: |
          echo "Launching unit tests"
          pytest tests/unit

      - name: Deploy integration test [staging environment]
        run: |
          dbx deploy --jobs=STAGING-telco-churn-sample-integration-test --environment=staging --files-only

      - name: Run integration test [staging environment]
        run: |
          dbx launch --job=STAGING-telco-churn-sample-integration-test --environment=staging --as-run-submit --trace